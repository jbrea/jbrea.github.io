@article{amodioTestingTwoCompeting2021,
 pdf           = {amodio_et_al.-2021-testing_two_competing_hypotheses_for_eurasian_jays.pdf},
 author        = {Amodio, Piero and Brea, Johanni and Farrar, Benjamin G. and Ostoji{\'c}, Ljerka and Clayton, Nicola S.},
 month         = {January},
 journal       = {Scientific Reports},
 title         = {Testing Two Competing Hypotheses for {{Eurasian}} Jays' Caching for the Future},
 publisher     = {{Springer Science and Business Media LLC}},
 number        = {1},
 doi           = {10.1038/s41598-020-80515-7},
 issn          = {2045-2322},
 year          = {2021},
 url           = {http://dx.doi.org/10.1038/s41598-020-80515-7},
 volume        = {11},
 html          = {http://dx.doi.org/10.1038/s41598-020-80515-7}
}

@inproceedings{bellecFittingSummaryStatistics2021,
 pdf           = {bellec_et_al.-2021-fitting_summary_statistics_of_neural_data_with_a_d.pdf},
 pages         = {18552--18563},
 booktitle     = {Advances in {{Neural Information Processing Systems}}},
 author        = {Bellec, Guillaume and Wang, Shuqi and Modirshanechi, Alireza and Brea, Johanni and Gerstner, Wulfram},
 editor        = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
 title         = {Fitting Summary Statistics of Neural Data with a Differentiable Spiking Network Simulator},
 publisher     = {Curran Associates, Inc.},
 code          = {https://github.com/EPFL-LCN/pub-bellec-wang-2021-sample-and-measure},
 year          = {2021},
 url           = {https://proceedings.neurips.cc/paper\_files/paper/2021/file/9a32ff36c65e8ba30915a21b7bd76506-Paper.pdf},
 volume        = {34},
 html          = {https://proceedings.neurips.cc/paper\_files/paper/2021/file/9a32ff36c65e8ba30915a21b7bd76506-Paper.pdf}
}

@article{breaComputationalModelsEpisodiclike2023,
 pdf           = {brea_et_al.-2023-computational_models_of_episodic-like_memory_in_fo.pdf},
 author        = {Brea, Johanni and Clayton, Nicola S. and Gerstner, Wulfram},
 month         = {May},
 journal       = {Nature Communications},
 title         = {Computational Models of Episodic-like Memory in Food-Caching Birds},
 publisher     = {{Springer Science and Business Media LLC1}},
 number        = {1},
 doi           = {10.1038/s41467-023-38570-x},
 code          = {https://github.com/jbrea/FoodCaching},
 issn          = {2041-1723},
 year          = {2023},
 url           = {http://dx.doi.org/10.1038/s41467-023-38570-x},
 volume        = {14},
 html          = {http://dx.doi.org/10.1038/s41467-023-38570-x}
}

@article{breaDoesComputationalNeuroscience2016,
 pdf           = {brea_and_gerstner-2016-does_computational_neuroscience_need_new_synaptic.pdf},
 pages         = {61--66},
 author        = {Brea, Johanni and Gerstner, Wulfram},
 month         = {October},
 journal       = {Current Opinion in Behavioral Sciences},
 title         = {Does Computational Neuroscience Need New Synaptic Learning Paradigms?},
 publisher     = {Elsevier BV},
 doi           = {10.1016/j.cobeha.2016.05.012},
 issn          = {2352-1546},
 year          = {2016},
 url           = {http://dx.doi.org/10.1016/j.cobeha.2016.05.012},
 volume        = {11},
 html          = {http://dx.doi.org/10.1016/j.cobeha.2016.05.012}
}

@article{breaMatchingRecallStorage2013,
 pdf           = {brea_et_al.-2013-matching_recall_and_storage_in_sequence_learning_w.pdf},
 pages         = {9565--9575},
 author        = {Brea, J. and Senn, W. and Pfister, J.-P.},
 month         = {June},
 journal       = {Journal of Neuroscience},
 title         = {Matching {{Recall}} and {{Storage}} in {{Sequence Learning}} with {{Spiking Neural Networks}}},
 publisher     = {Society for Neuroscience},
 number        = {23},
 doi           = {10.1523/jneurosci.4098-12.2013},
 issn          = {1529-2401},
 year          = {2013},
 url           = {http://dx.doi.org/10.1523/jneurosci.4098-12.2013},
 volume        = {33},
 html          = {http://dx.doi.org/10.1523/jneurosci.4098-12.2013}
}

@article{breaMemoryAugmentedReinforcementLearning2019,
 pdf           = {brea_and_gerstner-2019-a_memory-augmented_reinforcement_learning_model_of.pdf},
 author        = {Brea, Johanni and Gerstner, Wulfram},
 year          = {2019},
 url           = {https://doi.org/10.32470/CCN.2019.1316-0},
 journal       = {2019 Conference on Cognitive Computational Neuroscience},
 html          = {https://doi.org/10.32470/CCN.2019.1316-0},
 title         = {A {{Memory-Augmented Reinforcement Learning Model}} of {{Food Caching Behaviour}} in {{Birds}}}
}

@misc{breaMLPGradientFlowGoingFlow2023a,
 pdf           = {brea_et_al.-2023-mlpgradientflow_going_with_the_flow_of_multilayer.pdf},
 shorttitle    = {{{MLPGradientFlow}}},
 author        = {Brea, Johanni and Martinelli, Flavio and {\c S}im{\c s}ek, Berfin and Gerstner, Wulfram},
 month         = {January},
 journal       = {arXiv.org},
 title         = {{{MLPGradientFlow}}: Going with the Flow of Multilayer Perceptrons (and Finding Minima Fast and Accurately)},
 langid        = {english},
 urldate       = {2024-10-08},
 year          = {2023},
 url           = {https://arxiv.org/abs/2301.10638v1},
 html          = {https://arxiv.org/abs/2301.10638v1},
 abstract      = {MLPGradientFlow is a software package to solve numerically the gradient flow differential equation \${\textbackslash}dot {\textbackslash}theta = -{\textbackslash}nabla {\textbackslash}mathcal L({\textbackslash}theta; {\textbackslash}mathcal D)\$, where \${\textbackslash}theta\$ are the parameters of a multi-layer perceptron, \${\textbackslash}mathcal D\$ is some data set, and \${\textbackslash}nabla {\textbackslash}mathcal L\$ is the gradient of a loss function. We show numerically that adaptive first- or higher-order integration methods based on Runge-Kutta schemes have better accuracy and convergence speed than gradient descent with the Adam optimizer. However, we find Newton's method and approximations like BFGS preferable to find fixed points (local and global minima of \${\textbackslash}mathcal L\$) efficiently and accurately. For small networks and data sets, gradients are usually computed faster than in pytorch and Hessian are computed at least \$5{\textbackslash}times\$ faster. Additionally, the package features an integrator for a teacher-student setup with bias-free, two-layer networks trained with standard Gaussian input in the limit of infinite data. The code is accessible at https://github.com/jbrea/MLPGradientFlow.jl.}
}

@article{breaNormativeTheoryForgetting2014,
 pdf           = {brea_et_al.-2014-a_normative_theory_of_forgetting_lessons_from_the.pdf},
 pages         = {e1003640},
 author        = {Brea, Johanni and Urbanczik, Robert and Senn, Walter},
 editor        = {Daunizeau, JeanEditor},
 month         = {June},
 journal       = {PLoS Computational Biology},
 title         = {A {{Normative Theory}} of {{Forgetting}}: {{Lessons}} from the {{Fruit Fly}}},
 publisher     = {Public Library of Science (PLoS)},
 number        = {6},
 doi           = {10.1371/journal.pcbi.1003640},
 issn          = {1553-7358},
 year          = {2014},
 url           = {http://dx.doi.org/10.1371/journal.pcbi.1003640},
 volume        = {10},
 html          = {http://dx.doi.org/10.1371/journal.pcbi.1003640}
}

@article{breaPrioritizedSweepingBetter2017,
 pdf           = {brea-2017-is_prioritized_sweeping_the_better_episodic_contro.pdf},
 keywords      = {Computer Science - Artificial Intelligence,Computer Science - Learning},
 author        = {Brea, Johanni},
 year          = {2017},
 url           = {https://arxiv.org/abs/1711.06677},
 month         = {November},
 journal       = {ArXiv e-prints},
 html          = {https://arxiv.org/abs/1711.06677},
 title         = {Is Prioritized Sweeping the Better Episodic Control?}
}

@article{breaProspectiveCodingSpiking2016,
 pdf           = {brea_et_al.-2016-prospective_coding_by_spiking_neurons.pdf},
 pages         = {e1005003},
 author        = {Brea, Johanni and Ga{\'a}l, Alexisz Tam{\'a}s and Urbanczik, Robert and Senn, Walter},
 editor        = {Latham, Peter E.Editor},
 month         = {June},
 journal       = {PLOS Computational Biology},
 title         = {Prospective {{Coding}} by {{Spiking Neurons}}},
 publisher     = {Public Library of Science (PLoS)},
 number        = {6},
 doi           = {10.1371/journal.pcbi.1005003},
 issn          = {1553-7358},
 year          = {2016},
 url           = {http://dx.doi.org/10.1371/journal.pcbi.1005003},
 volume        = {12},
 html          = {http://dx.doi.org/10.1371/journal.pcbi.1005003}
}

@article{breaRememberingWhenHebbian2022,
 pdf           = {brea_et_al.-2022-remembering_the_“when”_hebbian_memory_models_for.pdf},
 author        = {Brea, Johanni and Modirshanechi, Alireza and Gerstner, Wulfram},
 month         = {November},
 journal       = {bioRxiv},
 title         = {Remembering the ``{{When}}'': {{Hebbian Memory Models}} for the {{Time}} of {{Past Events}}},
 publisher     = {Cold Spring Harbor Laboratory},
 doi           = {10.1101/2022.11.28.518209},
 year          = {2022},
 url           = {http://dx.doi.org/10.1101/2022.11.28.518209},
 html          = {http://dx.doi.org/10.1101/2022.11.28.518209}
}

@incollection{breaSequenceLearningHidden2011,
 pdf           = {brea_et_al.-2011-sequence_learning_with_hidden_units_in_spiking_neu.pdf},
 pages         = {1422--1430},
 publisher     = {Curran Associates, Inc.},
 booktitle     = {Advances in {{Neural Information Processing Systems}} 24},
 author        = {Brea, Johanni and Senn, Walter and Pfister, Jean-Pascal},
 editor        = {{Shawe-Taylor}, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
 year          = {2011},
 url           = {http://papers.nips.cc/paper/4383-sequence-learning-with-hidden-units-in-spiking-neural-networks},
 html          = {http://papers.nips.cc/paper/4383-sequence-learning-with-hidden-units-in-spiking-neural-networks},
 title         = {Sequence Learning with Hidden Units in Spiking Neural Networks}
}

@article{colomboAlgorithmicCompositionMelodies2016,
 pdf           = {colombo_et_al.-2016-algorithmic_composition_of_melodies_with_deep_recu.pdf},
 keywords      = {Computer Science - Learning,Statistics - Machine Learning},
 author        = {Colombo, F. and Muscinelli, S. P. and Seeholzer, A. and Brea, J. and Gerstner, W.},
 year          = {2016},
 url           = {https://arxiv.org/abs/1606.07251},
 month         = {June},
 journal       = {ArXiv e-prints},
 html          = {https://arxiv.org/abs/1606.07251},
 title         = {Algorithmic {{Composition}} of {{Melodies}} with {{Deep Recurrent Neural Networks}}}
}

@article{colomboLearningGenerateMusic2018,
 pdf           = {colombo_et_al.-2018-learning_to_generate_music_with_bachprop.pdf},
 keywords      = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
 author        = {Colombo, Florian and Brea, Johanni and Gerstner, Wulfram},
 year          = {2018},
 url           = {https://arxiv.org/abs/1812.06669},
 month         = {December},
 journal       = {arXiv e-prints},
 html          = {https://arxiv.org/abs/1812.06669},
 title         = {Learning to {{Generate Music}} with {{BachProp}}}
}

@inproceedings{corneilEfficientModelBased2018,
 pdf           = {corneil_et_al.-2018-efficient_model–based_deep_reinforcement_learning.pdf},
 pages         = {1057--1066},
 booktitle     = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
 author        = {Corneil, Dane and Gerstner, Wulfram and Brea, Johanni},
 editor        = {Dy, Jennifer and Krause, Andreas},
 month         = {July},
 title         = {Efficient {{Model}}--{{Based Deep Reinforcement Learning}} with {{Variational State Tabulation}}},
 publisher     = {PMLR},
 address       = {Stockholmsm{\"a}ssan, Stockholm Sweden},
 series        = {Proceedings of {{Machine Learning Research}}},
 year          = {2018},
 url           = {http://proceedings.mlr.press/v80/corneil18a.html},
 volume        = {80},
 html          = {http://proceedings.mlr.press/v80/corneil18a.html}
}

@article{fairbrotherGaussianProcessesjlNonparametricBayes2018,
 pdf           = {fairbrother_et_al.-2018-gaussianprocesses.jl_a_nonparametric_bayes_packag.pdf},
 author        = {Fairbrother, Jamie and Nemeth, Christopher and Rischard, Maxime and Brea, Johanni},
 year          = {2018},
 url           = {https://arxiv.org/abs/1812.09064},
 journal       = {arXiv e-prints},
 html          = {https://arxiv.org/abs/1812.09064},
 title         = {{{GaussianProcesses}}.Jl: {{A Nonparametric Bayes}} Package for the {{Julia Language}}}
}

@article{gerstnerEligibilityTracesPlasticity2018,
 pdf           = {gerstner_et_al.-2018-eligibility_traces_and_plasticity_on_behavioral_ti.pdf},
 pages         = {53},
 author        = {Gerstner, Wulfram and Lehmann, Marco and Liakoni, Vasiliki and Corneil, Dane and Brea, Johanni},
 month         = {July},
 journal       = {Frontiers in Neural Circuits},
 title         = {Eligibility {{Traces}} and {{Plasticity}} on {{Behavioral Time Scales}}: {{Experimental Support}} of {{NeoHebbian Three-Factor Learning Rules}}},
 publisher     = {Frontiers Media SA},
 doi           = {10.3389/fncir.2018.00053},
 issn          = {1662-5110},
 year          = {2018},
 url           = {http://dx.doi.org/10.3389/fncir.2018.00053},
 volume        = {12},
 html          = {http://dx.doi.org/10.3389/fncir.2018.00053}
}

@misc{gotmareDecouplingBackpropagationUsing2018,
 pdf           = {gotmare_et_al.-2018-decoupling_backpropagation_using_constrained_optim.pdf},
 author        = {Gotmare, Akhilesh and Thomas, Valentin and Brea, Johanni and Jaggi, Martin},
 year          = {2018},
 annotation    = {Published: ICML 2018, Efficient Credit Assignment in Deep Learning and Reinforcement Learning Workshop},
 url           = {https://openreview.net/forum?id=BygR79WfWm},
 html          = {https://openreview.net/forum?id=BygR79WfWm},
 title         = {Decoupling {{Backpropagation}} Using {{Constrained Optimization Methods}}}
}

@article{gruazMeritsCuriositySimulation2024,
 pdf           = {gruaz_et_al.-2024-merits_of_curiosity_a_simulation_study.pdf},
 publisher     = {Center for Open Science},
 doi           = {10.31234/osf.io/evm9n},
 author        = {Gruaz, Lucas and Modirshanechi, Alireza and Brea, Johanni},
 year          = {2024},
 url           = {http://dx.doi.org/10.31234/osf.io/evm9n},
 month         = {September},
 html          = {http://dx.doi.org/10.31234/osf.io/evm9n},
 title         = {Merits of Curiosity: A Simulation Study}
}

@inproceedings{iatropoulosKernelMemoryNetworks2022,
 pdf           = {iatropoulos_et_al.-2022-kernel_memory_networks_a_unifying_framework_for_m.pdf},
 pages         = {35326--35338},
 booktitle     = {Advances in {{Neural Information Processing Systems}}},
 author        = {Iatropoulos, Georgios and Brea, Johanni and Gerstner, Wulfram},
 editor        = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
 title         = {Kernel {{Memory Networks}}: {{A Unifying Framework}} for {{Memory Modeling}}},
 publisher     = {Curran Associates, Inc.},
 year          = {2022},
 url           = {https://proceedings.neurips.cc/paper{\textbackslash}\_files/paper/2022/file/e55d081280e79e714debf2902e18eb69-Paper-Conference.pdf},
 volume        = {35},
 html          = {https://proceedings.neurips.cc/paper{\textbackslash}\_files/paper/2022/file/e55d081280e79e714debf2902e18eb69-Paper-Conference.pdf}
}

@article{iatropoulosTwofactorSynapticConsolidation2024,
 pdf           = {iatropoulos_et_al.-2024-two-factor_synaptic_consolidation_reconciles_robus.pdf},
 publisher     = {Cold Spring Harbor Laboratory},
 doi           = {10.1101/2024.07.23.604787},
 author        = {Iatropoulos, Georgios and Gerstner, Wulfram and Brea, Johanni},
 year          = {2024},
 url           = {http://dx.doi.org/10.1101/2024.07.23.604787},
 month         = {July},
 html          = {http://dx.doi.org/10.1101/2024.07.23.604787},
 title         = {Two-Factor Synaptic Consolidation Reconciles Robust Memory with Pruning and Homeostatic Scaling}
}

@article{illingBiologicallyPlausibleDeep2019,
 pdf           = {illing_et_al.-2019-biologically_plausible_deep_learning_—_but_how_far.pdf},
 pages         = {90--101},
 author        = {Illing, Bernd and Gerstner, Wulfram and Brea, Johanni},
 month         = {October},
 journal       = {Neural Networks},
 title         = {Biologically Plausible Deep Learning --- {{But}} How Far Can We Go with Shallow Networks?},
 publisher     = {Elsevier BV},
 doi           = {10.1016/j.neunet.2019.06.001},
 issn          = {0893-6080},
 year          = {2019},
 url           = {http://dx.doi.org/10.1016/j.neunet.2019.06.001},
 volume        = {118},
 html          = {http://dx.doi.org/10.1016/j.neunet.2019.06.001}
}

@misc{illingLocalizedRandomProjections2018,
 pdf           = {illing_et_al.-2018-localized_random_projections_challenge_benchmarks.pdf},
 author        = {Illing, Bernd and Gerstner, Wulfram and Brea, Johanni},
 year          = {2018},
 url           = {https://openreview.net/forum?id=SJeT\_oRcY7},
 html          = {https://openreview.net/forum?id=SJeT\_oRcY7},
 title         = {Localized Random Projections Challenge Benchmarks for Bio-Plausible Deep Learning}
}

@article{liakoniBrainSignalsSurpriseActorCritic2022,
 pdf           = {liakoni_et_al.-2022-brain_signals_of_a_surprise-actor-critic_model_ev.pdf},
 pages         = {118780},
 shorttitle    = {Brain Signals of a {{Surprise-Actor-Critic}} Model},
 author        = {Liakoni, Vasiliki and Lehmann, Marco P. and Modirshanechi, Alireza and Brea, Johanni and Lutti, Antoine and Gerstner, Wulfram and Preuschoff, Kerstin},
 month         = {February},
 journal       = {NeuroImage},
 title         = {Brain Signals of a {{Surprise-Actor-Critic}} Model: {{Evidence}} for Multiple Learning Modules in Human Decision Making},
 doi           = {10.1016/j.neuroimage.2021.118780},
 keywords      = {Behavior,fMRI,Human learning,Reinforcement learning,Sequential decision making,Surprise},
 issn          = {1053-8119},
 urldate       = {2024-09-15},
 year          = {2022},
 url           = {https://www.sciencedirect.com/science/article/pii/S1053811921010521},
 volume        = {246},
 html          = {https://www.sciencedirect.com/science/article/pii/S1053811921010521},
 abstract      = {Learning how to reach a reward over long series of actions is a remarkable capability of humans, and potentially guided by multiple parallel learning modules. Current brain imaging of learning modules is limited by (i) simple experimental paradigms, (ii) entanglement of brain signals of different learning modules, and (iii) a limited number of computational models considered as candidates for explaining behavior. Here, we address these three limitations and (i) introduce a complex sequential decision making task with surprising events that allows us to (ii) dissociate correlates of reward prediction errors from those of surprise in functional magnetic resonance imaging (fMRI); and (iii) we test behavior against a large repertoire of model-free, model-based, and hybrid reinforcement learning algorithms, including a novel surprise-modulated actor-critic algorithm. Surprise, derived from an approximate Bayesian approach for learning the world-model, is extracted in our algorithm from a state prediction error. Surprise is then used to modulate the learning rate of a model-free actor, which itself learns via the reward prediction error from model-free value estimation by the critic. We find that action choices are well explained by pure model-free policy gradient, but reaction times and neural data are not. We identify signatures of both model-free and surprise-based learning signals in blood oxygen level dependent (BOLD) responses, supporting the existence of multiple parallel learning modules in the brain. Our results extend previous fMRI findings to a multi-step setting and emphasize the role of policy gradient and surprise signalling in human learning.}
}

@article{liakoniLearningVolatileEnvironments2021,
 pdf           = {liakoni_et_al.-2021-learning_in_volatile_environments_with_the_bayes_f.pdf},
 pages         = {269--340},
 author        = {Liakoni, Vasiliki and Modirshanechi, Alireza and Gerstner, Wulfram and Brea, Johanni},
 month         = {February},
 journal       = {Neural Computation},
 title         = {Learning in {{Volatile Environments With}} the {{Bayes Factor Surprise}}},
 publisher     = {MIT Press},
 number        = {2},
 doi           = {10.1162/neco_a_01352},
 issn          = {1530-888X},
 year          = {2021},
 url           = {http://dx.doi.org/10.1162/neco\_a\_01352},
 volume        = {33},
 html          = {http://dx.doi.org/10.1162/neco\_a\_01352}
}

@article{mannaBehavioralIndividualityConsequence2024,
 pdf           = {manna_et_al.-2024-behavioral_individuality_is_a_consequence_of_exper.pdf},
 publisher     = {Cold Spring Harbor Laboratory},
 doi           = {10.1101/2024.08.30.610528},
 author        = {Manna, Riddha and Brea, Johanni and Braga, Gon{\c c}alo Vasconcelos and Modirshanechi, Alireza and Tomi{\'c}, Ivan and Jak{\v s}i{\'c}, Ana Marija},
 year          = {2024},
 url           = {http://dx.doi.org/10.1101/2024.08.30.610528},
 month         = {September},
 html          = {http://dx.doi.org/10.1101/2024.08.30.610528},
 title         = {Behavioral Individuality Is a Consequence of Experience, Genetics and Learning}
}

@article{mesnardDeepLearningSpiking2016,
 pdf           = {mesnard_et_al.-2016-towards_deep_learning_with_spiking_neurons_in_ener.pdf},
 keywords      = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
 author        = {Mesnard, T. and Gerstner, W. and Brea, J.},
 year          = {2016},
 url           = {https://arxiv.org/abs/1612.03214},
 month         = {December},
 journal       = {ArXiv e-prints},
 html          = {https://arxiv.org/abs/1612.03214},
 title         = {Towards Deep Learning with Spiking Neurons in Energy Based Models with Contrastive {{Hebbian}} Plasticity}
}

@article{modirshanechiTaxonomySurpriseDefinitions2022,
 pdf           = {modirshanechi_et_al.-2022-a_taxonomy_of_surprise_definitions.pdf},
 pages         = {102712},
 author        = {Modirshanechi, Alireza and Brea, Johanni and Gerstner, Wulfram},
 month         = {September},
 journal       = {Journal of Mathematical Psychology},
 title         = {A Taxonomy of Surprise Definitions},
 publisher     = {Elsevier BV},
 doi           = {10.1016/j.jmp.2022.102712},
 issn          = {0022-2496},
 year          = {2022},
 url           = {http://dx.doi.org/10.1016/j.jmp.2022.102712},
 volume        = {110},
 html          = {http://dx.doi.org/10.1016/j.jmp.2022.102712}
}

@article{muscinelliExponentiallyLongOrbits2017,
 pdf           = {muscinelli_et_al.-2017-exponentially_long_orbits_in_hopfield_neural_netwo.pdf},
 pages         = {458--484},
 author        = {Muscinelli, Samuel P. and Gerstner, Wulfram and Brea, Johanni},
 month         = {February},
 journal       = {Neural Computation},
 title         = {Exponentially {{Long Orbits}} in {{Hopfield Neural Networks}}},
 publisher     = {MIT Press - Journals},
 number        = {2},
 doi           = {10.1162/neco_a_00919},
 issn          = {1530-888X},
 year          = {2017},
 url           = {http://dx.doi.org/10.1162/NECO\_a\_00919},
 volume        = {29},
 html          = {http://dx.doi.org/10.1162/NECO\_a\_00919}
}

@misc{schmutzEmergentRatebasedDynamics2024,
 pdf           = {schmutz_et_al.-2024-emergent_rate-based_dynamics_in_duplicate-free_pop.pdf},
 primaryClass  = {q-bio},
 author        = {Schmutz, Valentin and Brea, Johanni and Gerstner, Wulfram},
 month         = {May},
 archivePrefix = {arXiv},
 title         = {Emergent Rate-Based Dynamics in Duplicate-Free Populations of Spiking Neurons},
 publisher     = {arXiv},
 number        = {arXiv:2303.05174},
 doi           = {10.48550/arXiv.2303.05174},
 keywords      = {Quantitative Biology - Neurons and Cognition},
 urldate       = {2024-10-01},
 eprint        = {2303.05174},
 year          = {2024},
 url           = {http://arxiv.org/abs/2303.05174},
 html          = {http://arxiv.org/abs/2303.05174},
 abstract      = {Can Spiking Neural Networks (SNNs) approximate the dynamics of Recurrent Neural Networks (RNNs)? Arguments in classical mean-field theory based on laws of large numbers provide a positive answer when each neuron in the network has many "duplicates", i.e. other neurons with almost perfectly correlated inputs. Using a disordered network model that guarantees the absence of duplicates, we show that duplicate-free SNNs can converge to RNNs, thanks to the concentration of measure phenomenon. This result reveals a general mechanism underlying the emergence of rate-based dynamics in large SNNs.}
}

@article{sennNeuronsThatRemember2015,
 pdf           = {senn_and_brea-2015-neurons_that_remember_how_we_got_there.pdf},
 pages         = {664--666},
 author        = {Senn, Walter and Brea, Johanni},
 month         = {February},
 journal       = {Neuron},
 title         = {Neurons That {{Remember How We Got There}}},
 publisher     = {Elsevier BV},
 number        = {4},
 doi           = {10.1016/j.neuron.2015.01.029},
 issn          = {0896-6273},
 year          = {2015},
 url           = {http://dx.doi.org/10.1016/j.neuron.2015.01.029},
 volume        = {85},
 html          = {http://dx.doi.org/10.1016/j.neuron.2015.01.029}
}

@inproceedings{simsekGeometryLossLandscape2021,
 pdf           = {simsek_et_al.-2021-geometry_of_the_loss_landscape_in_overparameterize.pdf},
 pages         = {9722--9732},
 booktitle     = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
 author        = {Simsek, Berfin and Ged, Fran{\c c}ois and Jacot, Arthur and Spadaro, Francesco and Hongler, Clement and Gerstner, Wulfram and Brea, Johanni},
 editor        = {Meila, Marina and Zhang, Tong},
 month         = {July},
 title         = {Geometry of the {{Loss Landscape}} in {{Overparameterized Neural Networks}}: {{Symmetries}} and {{Invariances}}},
 publisher     = {PMLR},
 series        = {Proceedings of {{Machine Learning Research}}},
 year          = {2021},
 url           = {https://proceedings.mlr.press/v139/simsek21a.html},
 volume        = {139},
 html          = {https://proceedings.mlr.press/v139/simsek21a.html}
}

@inproceedings{simsekShouldParameterizedStudent2023,
 pdf           = {simsek_et_al.-2023-should_under-parameterized_student_networks_copy_o.pdf},
 booktitle     = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
 author        = {Simsek, Berfin and Bendjeddou, Amire and Gerstner, Wulfram and Brea, Johanni},
 year          = {2023},
 url           = {https://openreview.net/forum?id=MG0mYskXN2},
 html          = {https://openreview.net/forum?id=MG0mYskXN2},
 title         = {Should {{Under-parameterized Student Networks Copy}} or {{Average Teacher Weights}}?}
}

@article{suraceChoiceMetricGradientbased2020,
 pdf           = {surace_et_al.-2020-on_the_choice_of_metric_in_gradient-based_theories.pdf},
 pages         = {e1007640},
 author        = {Surace, Simone Carlo and Pfister, Jean-Pascal and Gerstner, Wulfram and Brea, Johanni},
 editor        = {Ouellette, FrancisEditor},
 month         = {April},
 journal       = {PLOS Computational Biology},
 title         = {On the Choice of Metric in Gradient-Based Theories of Brain Function},
 publisher     = {Public Library of Science (PLoS)},
 number        = {4},
 doi           = {10.1371/journal.pcbi.1007640},
 keywords      = {Quantitative Biology - Neurons and Cognition},
 issn          = {1553-7358},
 year          = {2020},
 url           = {http://dx.doi.org/10.1371/journal.pcbi.1007640},
 volume        = {16},
 html          = {http://dx.doi.org/10.1371/journal.pcbi.1007640}
}

@article{vianoNeuralNIDRules2022,
 pdf           = {viano_and_brea-2022-neural_nid_rules.pdf},
 pages         = {arXiv:2202.06036},
 author        = {Viano, Luca and Brea, Johanni},
 month         = {February},
 journal       = {arXiv e-prints},
 title         = {Neural {{NID Rules}}},
 doi           = {10.48550/arXiv.2202.06036},
 keywords      = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 year          = {2022},
 url           = {https://arxiv.org/abs/2202.06036},
 html          = {https://arxiv.org/abs/2202.06036}
}
